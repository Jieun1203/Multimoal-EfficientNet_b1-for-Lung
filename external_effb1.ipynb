{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "based-montana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from clinical_data_loader.ipynb\n",
      "importing Jupyter notebook from clinical_data_loader_external_test.ipynb\n",
      "importing Jupyter notebook from clinical_data_loader_external_validation.ipynb\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob, os, re\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import  torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import import_ipynb\n",
    "# from models_3ch import MyEffientnet_b1\n",
    "from clinical_data_loader import ImageDataset \n",
    "from clinical_data_loader_external_test import ImageDataset as t_ImageDataset\n",
    "from clinical_data_loader_external_validation import ImageDataset as v_ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "welsh-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlikely-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "palestinian-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "matched-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random_seed = 123 #123\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "norman-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_transforms = {\n",
    "    'train_aug' : transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(0.5), #0.5\n",
    "        transforms.RandomVerticalFlip(0.5), #0.5\n",
    "        transforms.CenterCrop(240),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "test_transforms={\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(240),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "progressive-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/3ch_img_minmax/img'\n",
    "label_pth = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/3ch_img_minmax/patient_information_clinical.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complex-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/3ch_img_minmax/result/final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cordless-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEffientnet_b1_clinical(nn.Module):\n",
    "    def __init__(self,out_features1, out_features2, out_features3, out_features4, out_features5, out_features6, \n",
    "                 out_features7, out_features8, model_name='efficientnet-b1',class_num=45,initfc_type='normal',gain=0.2):\n",
    "        super(MyEffientnet_b1_clinical, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.clinical_fc1 = nn.Linear(3, out_features5) #(3, 32)\n",
    "        self.clinical_fc2 = nn.Linear(out_features5, out_features6) #(32, 256)\n",
    "        self.clinical_fc3 = nn.Linear(out_features6, out_features7) #(256, 1024)\n",
    "        self.clinical_fc4 = nn.Linear(out_features7, out_features8) #(1024, 1280)\n",
    "\n",
    "        \n",
    "        \n",
    "        model = EfficientNet.from_pretrained(model_name)\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(1280 + out_features8, out_features1) #1280\n",
    "        self.fc2 = nn.Linear(out_features1, out_features2)\n",
    "        self.fc3 = nn.Linear(out_features2, out_features3)\n",
    "        self.fc4 = nn.Linear(out_features3, out_features4)\n",
    "        self.fc5 = nn.Linear(out_features4, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm1d(1280 + out_features8)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(out_features1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(out_features2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(out_features3)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(out_features4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if hasattr(self.fc1, 'bias') and self.fc1.bias is not None:\n",
    "            nn.init.constant_(self.fc1.bias.data, 0.0)\n",
    "        if initfc_type == 'normal':\n",
    "            nn.init.normal_(self.fc1.weight.data, 0.0, gain)\n",
    "        elif initfc_type == 'xavier':\n",
    "            nn.init.xavier_normal_(self.fc1.weight.data, gain=gain)\n",
    "        elif initfc_type == 'kaiming':\n",
    "            nn.init.kaiming_normal_(self.fc1.weight.data, a=0, mode='fan_in')\n",
    "        elif initfc_type == 'orthogonal':\n",
    "            nn.init.orthogonal_(self.fc1.weight.data, gain=gain)\n",
    "\n",
    "\n",
    "    def forward(self,x, c):\n",
    "        x = self.model.extract_features(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        c = self.clinical_fc1(c)\n",
    "        c = self.clinical_fc2(c)\n",
    "        c = self.clinical_fc3(c)\n",
    "        c = self.clinical_fc4(c)\n",
    "\n",
    "        x = torch.cat((x, c), 1)\n",
    "        \n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "completed-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_root = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/3ch_img_minmax/final_patient_index.xlsx'\n",
    "idx_df = pd.read_excel(idx_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "difficult-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = idx_df['train_number'].to_numpy()\n",
    "val_idx = idx_df['val_number'].dropna().to_numpy()\n",
    "test_idx = idx_df['test_number'].dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "analyzed-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(img_root, train_idx, label_pth, 'train', transform = train_transforms['train_aug'])\n",
    "val_dataset = ImageDataset(img_root, val_idx, label_pth, 'val', transform = test_transforms['val'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=0, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                          num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "genetic-breakfast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bored-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fossil-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img_root = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/external_validationset/external_test/png'\n",
    "t_label_pth = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/external_validationset/external_test/GGN_TKI_clinical.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "transsexual-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_test_dataset = t_ImageDataset(t_img_root, t_label_pth, 'test', transform = test_transforms['val'])\n",
    "ex_test_loader = torch.utils.data.DataLoader(ex_test_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "standing-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_img_root = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/external_validationset/external_validation/png/ct'\n",
    "v_label_pth = '/home/SMC_data_2101xx_EGFR_deeplearning/EGFR_deep_learning/external_validationset/external_validation/external_validation_clinical.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "violent-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_val_dataset = v_ImageDataset(v_img_root, v_label_pth, 'val', transform = test_transforms['val'])\n",
    "ex_val_loader = torch.utils.data.DataLoader(ex_val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "presidential-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(img_root, test_idx, label_pth, 'test', transform = test_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "basic-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test_dataset = torch.utils.data.ConcatDataset([test_dataset, ex_val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "returning-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(f_test_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "inner-appreciation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyEffientnet_b1_clinical(\n",
       "  (clinical_fc1): Linear(in_features=3, out_features=16, bias=True)\n",
       "  (clinical_fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (clinical_fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (clinical_fc4): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (model): EfficientNet(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          16, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (16): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (17): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (18): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (19): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (20): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (21): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (22): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1920, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1920, 1920, kernel_size=(3, 3), stride=(1, 1), groups=1920, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1920, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1920, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          80, 1920, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (fc1): Linear(in_features=1312, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc5): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (batchnorm): BatchNorm1d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyEffientnet_b1_clinical(512, 512, 256, 32, 16, 16, 16, 32)\n",
    "my_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "handled-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = torch.FloatTensor([0.6, 0.4]).cuda()\n",
    "criterion = nn.CrossEntropyLoss(class_weight)\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=9e-5, weight_decay=1e-5)    \n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.99 ** epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abstract-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_check = np.array([])\n",
    "train_auc_check = np.array([])\n",
    "val_loss_check = np.array([])\n",
    "val_auc_check = np.array([])\n",
    "val_acc_check = np.array([])\n",
    "test_acc_check = np.array([])\n",
    "test_auc_check = np.array([])\n",
    "ex_test_acc_check = np.array([])\n",
    "ex_test_auc_check = np.array([])\n",
    "ex_val_auc_check = np.array([])\n",
    "ex_val_acc_check = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "arabic-anaheim",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.53it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 0, validation_loss : 0.694312\n",
      "Test --------- epoch : 0, test_auc : 0.511765\n",
      "Test --------- epoch : 0, test_auc : 0.511765\n",
      "Epoch:[0]/[30]\ttrain auc: 0.51 acc: 0.55\t val auc: 0.37 acc: 0.39\n",
      "test auc: 0.51 acc: 0.49\tex_test auc: 0.67 acc: 0.29\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.06it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.26it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test --------- epoch : 1, test_auc : 0.517647\n",
      "Epoch:[1]/[30]\ttrain auc: 0.63 acc: 0.58\t val auc: 0.29 acc: 0.35\n",
      "test auc: 0.52 acc: 0.38\tex_test auc: 0.66 acc: 0.18\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.15it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.27it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test --------- epoch : 2, test_auc : 0.527451\n",
      "Epoch:[2]/[30]\ttrain auc: 0.68 acc: 0.66\t val auc: 0.35 acc: 0.39\n",
      "test auc: 0.53 acc: 0.36\tex_test auc: 0.69 acc: 0.20\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.16it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.26it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test --------- epoch : 3, test_auc : 0.576471\n",
      "Epoch:[3]/[30]\ttrain auc: 0.79 acc: 0.71\t val auc: 0.36 acc: 0.35\n",
      "test auc: 0.58 acc: 0.47\tex_test auc: 0.69 acc: 0.24\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.10it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test --------- epoch : 4, test_auc : 0.592157\n",
      "Epoch:[4]/[30]\ttrain auc: 0.72 acc: 0.66\t val auc: 0.35 acc: 0.35\n",
      "test auc: 0.59 acc: 0.51\tex_test auc: 0.68 acc: 0.30\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.11it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.21it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 5, validation_loss : 0.699101\n",
      "Test --------- epoch : 5, test_auc : 0.625490\n",
      "Test --------- epoch : 5, test_auc : 0.625490\n",
      "Epoch:[5]/[30]\ttrain auc: 0.81 acc: 0.70\t val auc: 0.39 acc: 0.39\n",
      "test auc: 0.63 acc: 0.57\tex_test auc: 0.65 acc: 0.33\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.04it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.21it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 6, validation_loss : 0.697683\n",
      "Test --------- epoch : 6, test_auc : 0.717647\n",
      "Test --------- epoch : 6, test_auc : 0.717647\n",
      "Epoch:[6]/[30]\ttrain auc: 0.87 acc: 0.74\t val auc: 0.45 acc: 0.39\n",
      "test auc: 0.72 acc: 0.55\tex_test auc: 0.65 acc: 0.36\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.04it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 7, validation_loss : 0.693527\n",
      "Test --------- epoch : 7, test_auc : 0.733333\n",
      "Test --------- epoch : 7, test_auc : 0.733333\n",
      "Epoch:[7]/[30]\ttrain auc: 0.93 acc: 0.78\t val auc: 0.50 acc: 0.35\n",
      "test auc: 0.73 acc: 0.49\tex_test auc: 0.63 acc: 0.24\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.47it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.39it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 8, validation_loss : 0.686316\n",
      "Test --------- epoch : 8, test_auc : 0.723529\n",
      "Epoch:[8]/[30]\ttrain auc: 0.93 acc: 0.83\t val auc: 0.58 acc: 0.48\n",
      "test auc: 0.72 acc: 0.43\tex_test auc: 0.55 acc: 0.23\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.24it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.58it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 9, validation_loss : 0.677554\n",
      "Test --------- epoch : 9, test_auc : 0.735294\n",
      "Test --------- epoch : 9, test_auc : 0.735294\n",
      "Epoch:[9]/[30]\ttrain auc: 0.91 acc: 0.79\t val auc: 0.70 acc: 0.52\n",
      "test auc: 0.74 acc: 0.47\tex_test auc: 0.45 acc: 0.20\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.08it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.26it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 10, validation_loss : 0.671628\n",
      "Test --------- epoch : 10, test_auc : 0.745098\n",
      "Test --------- epoch : 10, test_auc : 0.745098\n",
      "Epoch:[10]/[30]\ttrain auc: 0.95 acc: 0.83\t val auc: 0.77 acc: 0.52\n",
      "test auc: 0.75 acc: 0.51\tex_test auc: 0.38 acc: 0.17\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.09it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 11, validation_loss : 0.665776\n",
      "Test --------- epoch : 11, test_auc : 0.768627\n",
      "Test --------- epoch : 11, test_auc : 0.768627\n",
      "Epoch:[11]/[30]\ttrain auc: 0.96 acc: 0.86\t val auc: 0.77 acc: 0.61\n",
      "test auc: 0.77 acc: 0.53\tex_test auc: 0.39 acc: 0.19\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.18it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.28it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --------- epoch : 12, validation_loss : 0.664447\n",
      "Test --------- epoch : 12, test_auc : 0.772549\n",
      "Test --------- epoch : 12, test_auc : 0.772549\n",
      "Epoch:[12]/[30]\ttrain auc: 0.97 acc: 0.87\t val auc: 0.78 acc: 0.61\n",
      "test auc: 0.77 acc: 0.53\tex_test auc: 0.38 acc: 0.24\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.12it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.30it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[13]/[30]\ttrain auc: 0.94 acc: 0.85\t val auc: 0.74 acc: 0.57\n",
      "test auc: 0.73 acc: 0.53\tex_test auc: 0.37 acc: 0.20\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.16it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.61it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[14]/[30]\ttrain auc: 0.96 acc: 0.90\t val auc: 0.73 acc: 0.48\n",
      "test auc: 0.66 acc: 0.49\tex_test auc: 0.35 acc: 0.20\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.65it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[15]/[30]\ttrain auc: 0.98 acc: 0.93\t val auc: 0.71 acc: 0.52\n",
      "test auc: 0.61 acc: 0.47\tex_test auc: 0.30 acc: 0.19\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.62it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[16]/[30]\ttrain auc: 0.95 acc: 0.86\t val auc: 0.70 acc: 0.48\n",
      "test auc: 0.58 acc: 0.40\tex_test auc: 0.26 acc: 0.18\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.58it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.63it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[17]/[30]\ttrain auc: 0.97 acc: 0.86\t val auc: 0.68 acc: 0.39\n",
      "test auc: 0.57 acc: 0.38\tex_test auc: 0.22 acc: 0.16\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.64it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[18]/[30]\ttrain auc: 0.98 acc: 0.90\t val auc: 0.63 acc: 0.43\n",
      "test auc: 0.59 acc: 0.38\tex_test auc: 0.17 acc: 0.16\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.57it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.61it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[19]/[30]\ttrain auc: 1.00 acc: 0.91\t val auc: 0.63 acc: 0.39\n",
      "test auc: 0.50 acc: 0.43\tex_test auc: 0.16 acc: 0.12\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.60it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[20]/[30]\ttrain auc: 0.99 acc: 0.94\t val auc: 0.57 acc: 0.35\n",
      "test auc: 0.47 acc: 0.36\tex_test auc: 0.14 acc: 0.12\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.55it/s]\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b5b6ce2be4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0menu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_test_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_test_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_test_clinical_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_number\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_test_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mex_test_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_test_x_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mex_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_test_y_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/SMC_data_2101xx_EGFR_deeplearning/notebook/external_training/final/clinical_data_loader_external_test.ipynb\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n",
      "\u001b[0;32m/home/SMC_data_2101xx_EGFR_deeplearning/notebook/external_training/final/clinical_data_loader_external_test.ipynb\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(fn, df)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2883\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2884\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"bad mode {repr(mode)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2885\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2886\u001b[0m         raise ValueError(\n\u001b[1;32m   2887\u001b[0m             \u001b[0;34m\"StringIO cannot be used to open an image. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    epoch_loss_train = 0.0\n",
    "    epoch_train_acc = 0.0\n",
    "    predicted_train_output = np.array([])\n",
    "    train_real = np.array([])\n",
    "    train_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "    my_model.train()\n",
    "    for enu, (train_x_batch, train_y_batch, train_clinical_batch, p) in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        train_x = Variable(train_x_batch).cuda()\n",
    "        train_y = Variable(train_y_batch).cuda()\n",
    "        train_clinical = Variable(train_clinical_batch).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_output = my_model(train_x, train_clinical)\n",
    "        train_epoch_loss = criterion(train_output, torch.max(train_y, 1)[1])\n",
    "\n",
    "        train_pred = np.argmax(train_output.detach().data.cpu().numpy(), axis = 1)\n",
    "        train_true = np.argmax(train_y.detach().data.cpu().numpy(), axis = 1)\n",
    "        predicted_train_output = np.append(predicted_train_output, train_pred)\n",
    "        train_real = np.append(train_real, train_true)\n",
    "        train_probability = np.append(train_probability, train_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "        train_epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_train += (train_epoch_loss.data.item() * len(train_x_batch))\n",
    "\n",
    "\n",
    "    del train_x_batch, train_y_batch, train_output\n",
    "    train_loss = epoch_loss_train / len(train_dataset)\n",
    "    train_acc = len(np.where(predicted_train_output == train_real)[0]) / len(predicted_train_output)\n",
    "    train_auc_score = roc_auc_score(train_real, train_probability[:, 1])\n",
    "    \n",
    "    train_acc_check = np.append(train_acc_check, train_acc)\n",
    "    train_auc_check = np.append(train_auc_check, train_auc_score)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        epoch_loss_val = 0.0\n",
    "        epoch_acc_val = 0.0\n",
    "        predicted_val_output = np.array([])\n",
    "        val_real = np.array([])\n",
    "        val_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "        my_model.eval()\n",
    "\n",
    "        for enu, (validation_x_batch, validation_y_batch, validation_clinical_batch, p) in enumerate(tqdm(val_loader)):\n",
    "            validation_x = Variable(validation_x_batch).cuda()\n",
    "            validation_y = Variable(validation_y_batch).cuda()\n",
    "            vaidation_clinical = Variable(validation_clinical_batch).cuda()\n",
    "\n",
    "            validation_output = my_model(validation_x, vaidation_clinical)\n",
    "            validation_epoch_loss = criterion(validation_output, torch.max(validation_y, 1)[1])\n",
    "\n",
    "            epoch_loss_val += (validation_epoch_loss.data.item() * len(validation_x_batch))\n",
    "\n",
    "            pred_val = np.argmax(validation_output.data.cpu().numpy(), axis = 1)\n",
    "            true_val = np.argmax(validation_y.data.cpu().numpy(), axis = 1)\n",
    "            predicted_val_output = np.append(predicted_val_output, pred_val)\n",
    "            val_real = np.append(val_real, true_val)\n",
    "            val_probability = np.append(val_probability, validation_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "        del validation_x_batch, validation_y_batch, validation_output\n",
    "        val_loss = epoch_loss_val / len(val_dataset)\n",
    "        val_acc = len(np.where(predicted_val_output == val_real)[0]) / len(predicted_val_output)\n",
    "        val_auc_score = roc_auc_score(val_real, val_probability[:, 1])\n",
    "        val_auc_check = np.append(val_auc_check, val_auc_score)\n",
    "        val_loss_check = np.append(val_loss_check, val_loss)\n",
    "        val_acc_check = np.append(val_acc_check, val_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        epoch_loss_test = 0.0\n",
    "        epoch_acc_test = 0.0\n",
    "        predicted_test_output = np.array([])\n",
    "        test_real = np.array([])\n",
    "        test_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "        my_model.eval()\n",
    "\n",
    "        for enu, (test_x_batch, test_y_batch, test_clinical_batch, p) in enumerate(tqdm(test_loader)):\n",
    "            test_x = Variable(test_x_batch).cuda()\n",
    "            test_y = Variable(test_y_batch).cuda()\n",
    "            test_clinical = Variable(test_clinical_batch).cuda()\n",
    "\n",
    "            test_output = my_model(test_x, test_clinical)\n",
    "            test_epoch_loss = criterion(test_output, torch.max(test_y, 1)[1])\n",
    "\n",
    "            epoch_loss_test += (test_epoch_loss.data.item() * len(test_x_batch))\n",
    "\n",
    "            pred_test = np.argmax(test_output.data.cpu().numpy(), axis = 1)\n",
    "            true_test = np.argmax(test_y.data.cpu().numpy(), axis = 1)\n",
    "            predicted_test_output = np.append(predicted_test_output, pred_test)\n",
    "            test_real = np.append(test_real, true_test)\n",
    "            test_probability = np.append(test_probability, test_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "        del test_x_batch, test_y_batch, test_output\n",
    "        test_loss = epoch_loss_test / len(test_dataset)\n",
    "        test_acc = len(np.where(predicted_test_output == test_real)[0]) / len(predicted_test_output)\n",
    "        test_auc_score = roc_auc_score(test_real, test_probability[:, 1])\n",
    "        test_auc_check = np.append(test_auc_check, test_auc_score)\n",
    "        test_acc_check = np.append(test_acc_check, test_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        ex_epoch_loss_test = 0.0\n",
    "        ex_epoch_acc_test = 0.0\n",
    "        ex_predicted_test_output = np.array([])\n",
    "        ex_test_real = np.array([])\n",
    "        ex_test_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "        my_model.eval()\n",
    "\n",
    "        for enu, (ex_test_x_batch, ex_test_y_batch, ex_test_clinical_batch, p_number) in enumerate(tqdm(ex_test_loader)):\n",
    "            ex_test_x = Variable(ex_test_x_batch).cuda()\n",
    "            ex_test_y = Variable(ex_test_y_batch).cuda()\n",
    "            ex_test_clinical = Variable(ex_test_clinical_batch).cuda()\n",
    "\n",
    "            ex_test_output = my_model(ex_test_x, ex_test_clinical)\n",
    "            ex_test_epoch_loss = criterion(ex_test_output, torch.max(ex_test_y, 1)[1])\n",
    "\n",
    "            ex_epoch_loss_test += (ex_test_epoch_loss.data.item() * len(ex_test_x_batch))\n",
    "\n",
    "            ex_pred_test = np.argmax(ex_test_output.data.cpu().numpy(), axis = 1)\n",
    "            ex_true_test = np.argmax(ex_test_y.data.cpu().numpy(), axis = 1)\n",
    "            ex_predicted_test_output = np.append(ex_predicted_test_output, ex_pred_test)\n",
    "            ex_test_real = np.append(ex_test_real, ex_true_test)\n",
    "            ex_test_probability = np.append(ex_test_probability, ex_test_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "        del ex_test_x_batch, ex_test_y_batch, ex_test_output\n",
    "        ex_test_loss = ex_epoch_loss_test / len(ex_test_dataset)\n",
    "        ex_test_acc = len(np.where(ex_predicted_test_output == ex_test_real)[0]) / len(ex_predicted_test_output)\n",
    "        ex_test_auc_score = roc_auc_score(ex_test_real, ex_test_probability[:, 1])\n",
    "        ex_test_auc_check = np.append(ex_test_auc_check, ex_test_auc_score)\n",
    "        ex_test_acc_check = np.append(ex_test_acc_check, ex_test_acc)\n",
    "       \n",
    "    \n",
    "    if val_auc_check[epoch] == val_auc_check.max():\n",
    "        print('Validation --------- epoch : {}, validation_loss : {:.6f}'.format(epoch, val_loss_check[epoch]))\n",
    "        print('Test --------- epoch : {}, test_auc : {:.6f}'.format(epoch, test_auc_check[epoch]))\n",
    "        print('early stopping point is here')\n",
    "#         torch.save(my_model.state_dict(), save_root + '/classification_checkpoint.pt')\n",
    "\n",
    "    if test_acc_check[epoch] >= 0.80:\n",
    "        conf_matrix = confusion_matrix(test_real, predicted_test_output)\n",
    "        print(conf_matrix)\n",
    "        sensitivity = conf_matrix[0, 0] / conf_matrix.sum(axis = 1)[0]\n",
    "        specificity = conf_matrix[1, 1] / conf_matrix.sum(axis = 1)[1]\n",
    "        print('sensitivity : ', sensitivity)\n",
    "        print('specificity : ', specificity)\n",
    "\n",
    "    if test_auc_check[epoch] == test_auc_check.max():\n",
    "        print('Test --------- epoch : {}, test_auc : {:.6f}'.format(epoch, test_auc_check[epoch]))\n",
    "\n",
    "    if val_loss_check[epoch] == val_loss_check.min():\n",
    "        test_value = test_auc_check[epoch]\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch:[{}]/[{}]\\t'\n",
    "          'train auc: {:.2f} '\n",
    "         'acc: {:.2f}\\t '\n",
    "          'val auc: {:.2f} '\n",
    "         'acc: {:.2f}\\n'\n",
    "         'test auc: {:.2f} '\n",
    "         'acc: {:.2f}\\t'\n",
    "           'ex_test auc: {:.2f} '\n",
    "         'acc: {:.2f}\\t'\n",
    "          .format(epoch, num_epoch, train_auc_score, train_acc, \n",
    "                                      val_auc_score, val_acc, test_auc_score, test_acc, \n",
    "                  ex_test_auc_score, ex_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-spread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-mechanism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-rapid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-jamaica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-deposit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-directive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-footwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-chicago",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-asbestos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-situation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-paragraph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-native",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-kansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-liabilities",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-pharmacy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for epoch in range(num_epoch):\n",
    "#     epoch_loss_train = 0.0\n",
    "#     epoch_train_acc = 0.0\n",
    "#     predicted_train_output = np.array([])\n",
    "#     train_real = np.array([])\n",
    "#     train_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "#     my_model.train()\n",
    "#     for enu, (train_x_batch, train_y_batch, train_clinical_batch) in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "#         train_x = Variable(train_x_batch).cuda()\n",
    "#         train_y = Variable(train_y_batch).cuda()\n",
    "#         train_clinical = Variable(train_clinical_batch).cuda()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         train_output = my_model(train_x, train_clinical)\n",
    "#         train_epoch_loss = criterion(train_output, torch.max(train_y, 1)[1])\n",
    "\n",
    "#         train_pred = np.argmax(train_output.detach().data.cpu().numpy(), axis = 1)\n",
    "#         train_true = np.argmax(train_y.detach().data.cpu().numpy(), axis = 1)\n",
    "#         predicted_train_output = np.append(predicted_train_output, train_pred)\n",
    "#         train_real = np.append(train_real, train_true)\n",
    "#         train_probability = np.append(train_probability, train_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "#         train_epoch_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss_train += (train_epoch_loss.data.item() * len(train_x_batch))\n",
    "\n",
    "\n",
    "#     del train_x_batch, train_y_batch, train_output\n",
    "#     train_loss = epoch_loss_train / len(train_dataset)\n",
    "#     train_acc = len(np.where(predicted_train_output == train_real)[0]) / len(predicted_train_output)\n",
    "#     train_auc_score = roc_auc_score(train_real, train_probability[:, 1])\n",
    "    \n",
    "#     train_acc_check = np.append(train_acc_check, train_acc)\n",
    "#     train_auc_check = np.append(train_auc_check, train_auc_score)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         epoch_loss_val = 0.0\n",
    "#         epoch_acc_val = 0.0\n",
    "#         predicted_val_output = np.array([])\n",
    "#         val_real = np.array([])\n",
    "#         val_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "#         my_model.eval()\n",
    "\n",
    "#         for enu, (validation_x_batch, validation_y_batch, validation_clinical_batch) in enumerate(tqdm(val_loader)):\n",
    "#             validation_x = Variable(validation_x_batch).cuda()\n",
    "#             validation_y = Variable(validation_y_batch).cuda()\n",
    "#             vaidation_clinical = Variable(validation_clinical_batch).cuda()\n",
    "\n",
    "#             validation_output = my_model(validation_x, vaidation_clinical)\n",
    "#             validation_epoch_loss = criterion(validation_output, torch.max(validation_y, 1)[1])\n",
    "\n",
    "#             epoch_loss_val += (validation_epoch_loss.data.item() * len(validation_x_batch))\n",
    "\n",
    "#             pred_val = np.argmax(validation_output.data.cpu().numpy(), axis = 1)\n",
    "#             true_val = np.argmax(validation_y.data.cpu().numpy(), axis = 1)\n",
    "#             predicted_val_output = np.append(predicted_val_output, pred_val)\n",
    "#             val_real = np.append(val_real, true_val)\n",
    "#             val_probability = np.append(val_probability, validation_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "#         del validation_x_batch, validation_y_batch, validation_output\n",
    "#         val_loss = epoch_loss_val / len(val_dataset)\n",
    "#         val_acc = len(np.where(predicted_val_output == val_real)[0]) / len(predicted_val_output)\n",
    "#         val_auc_score = roc_auc_score(val_real, val_probability[:, 1])\n",
    "#         val_auc_check = np.append(val_auc_check, val_auc_score)\n",
    "#         val_loss_check = np.append(val_loss_check, val_loss)\n",
    "#         val_acc_check = np.append(val_acc_check, val_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     with torch.no_grad():\n",
    "#         epoch_loss_test = 0.0\n",
    "#         epoch_acc_test = 0.0\n",
    "#         predicted_test_output = np.array([])\n",
    "#         test_real = np.array([])\n",
    "#         test_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "#         my_model.eval()\n",
    "\n",
    "#         for enu, (test_x_batch, test_y_batch, test_clinical_batch) in enumerate(tqdm(test_loader)):\n",
    "#             test_x = Variable(test_x_batch).cuda()\n",
    "#             test_y = Variable(test_y_batch).cuda()\n",
    "#             test_clinical = Variable(test_clinical_batch).cuda()\n",
    "\n",
    "#             test_output = my_model(test_x, test_clinical)\n",
    "#             test_epoch_loss = criterion(test_output, torch.max(test_y, 1)[1])\n",
    "\n",
    "#             epoch_loss_test += (test_epoch_loss.data.item() * len(test_x_batch))\n",
    "\n",
    "#             pred_test = np.argmax(test_output.data.cpu().numpy(), axis = 1)\n",
    "#             true_test = np.argmax(test_y.data.cpu().numpy(), axis = 1)\n",
    "#             predicted_test_output = np.append(predicted_test_output, pred_test)\n",
    "#             test_real = np.append(test_real, true_test)\n",
    "#             test_probability = np.append(test_probability, test_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "#         del test_x_batch, test_y_batch, test_output\n",
    "#         test_loss = epoch_loss_test / len(test_dataset)\n",
    "#         test_acc = len(np.where(predicted_test_output == test_real)[0]) / len(predicted_test_output)\n",
    "#         test_auc_score = roc_auc_score(test_real, test_probability[:, 1])\n",
    "#         test_auc_check = np.append(test_auc_check, test_auc_score)\n",
    "#         test_acc_check = np.append(test_acc_check, test_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "#     with torch.no_grad():\n",
    "#         ex_epoch_loss_test = 0.0\n",
    "#         ex_epoch_acc_test = 0.0\n",
    "#         ex_predicted_test_output = np.array([])\n",
    "#         ex_test_real = np.array([])\n",
    "#         ex_test_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "#         my_model.eval()\n",
    "\n",
    "#         for enu, (ex_test_x_batch, ex_test_y_batch, ex_test_clinical_batch, p_number) in enumerate(tqdm(ex_test_loader)):\n",
    "#             ex_test_x = Variable(ex_test_x_batch).cuda()\n",
    "#             ex_test_y = Variable(ex_test_y_batch).cuda()\n",
    "#             ex_test_clinical = Variable(ex_test_clinical_batch).cuda()\n",
    "\n",
    "#             ex_test_output = my_model(ex_test_x, ex_test_clinical)\n",
    "#             ex_test_epoch_loss = criterion(ex_test_output, torch.max(ex_test_y, 1)[1])\n",
    "\n",
    "#             ex_epoch_loss_test += (ex_test_epoch_loss.data.item() * len(ex_test_x_batch))\n",
    "\n",
    "#             ex_pred_test = np.argmax(ex_test_output.data.cpu().numpy(), axis = 1)\n",
    "#             ex_true_test = np.argmax(ex_test_y.data.cpu().numpy(), axis = 1)\n",
    "#             ex_predicted_test_output = np.append(ex_predicted_test_output, ex_pred_test)\n",
    "#             ex_test_real = np.append(ex_test_real, ex_true_test)\n",
    "#             ex_test_probability = np.append(ex_test_probability, ex_test_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "#         del ex_test_x_batch, ex_test_y_batch, ex_test_output\n",
    "#         ex_test_loss = ex_epoch_loss_test / len(ex_test_dataset)\n",
    "#         ex_test_acc = len(np.where(ex_predicted_test_output == ex_test_real)[0]) / len(ex_predicted_test_output)\n",
    "#         ex_test_auc_score = roc_auc_score(ex_test_real, ex_test_probability[:, 1])\n",
    "#         ex_test_auc_check = np.append(ex_test_auc_check, ex_test_auc_score)\n",
    "#         ex_test_acc_check = np.append(ex_test_acc_check, ex_test_acc)\n",
    "\n",
    "        \n",
    "#     with torch.no_grad():\n",
    "#         ex_epoch_loss_val = 0.0\n",
    "#         ex_epoch_acc_val = 0.0\n",
    "#         ex_predicted_val_output = np.array([])\n",
    "#         ex_val_real = np.array([])\n",
    "#         ex_val_probability = np.array([]).reshape(0, 2)\n",
    "\n",
    "#         my_model.eval()\n",
    "\n",
    "#         for enu, (ex_validation_x_batch, ex_validation_y_batch, ex_validation_clinical_batch) in enumerate(tqdm(ex_val_loader)):\n",
    "#             ex_validation_x = Variable(ex_validation_x_batch).cuda()\n",
    "#             ex_validation_y = Variable(ex_validation_y_batch).cuda()\n",
    "#             ex_validation_clinical = Variable(ex_validation_clinical_batch).cuda()\n",
    "\n",
    "#             ex_validation_output = my_model(ex_validation_x, ex_validation_clinical)\n",
    "#             ex_validation_epoch_loss = criterion(ex_validation_output, torch.max(ex_validation_y, 1)[1])\n",
    "\n",
    "#             ex_epoch_loss_val += (ex_validation_epoch_loss.data.item() * len(ex_validation_x_batch))\n",
    "\n",
    "#             ex_pred_val = np.argmax(ex_validation_output.data.cpu().numpy(), axis = 1)\n",
    "#             ex_true_val = np.argmax(ex_validation_y.data.cpu().numpy(), axis = 1)\n",
    "#             ex_predicted_val_output = np.append(ex_predicted_val_output, ex_pred_val)\n",
    "#             ex_val_real = np.append(ex_val_real, ex_true_val)\n",
    "#             ex_val_probability = np.append(ex_val_probability, ex_validation_output.detach().data.cpu().numpy(), axis = 0)\n",
    "\n",
    "\n",
    "#         del ex_validation_x_batch, ex_validation_y_batch, ex_validation_output\n",
    "#         ex_val_loss = ex_epoch_loss_val / len(ex_val_dataset)\n",
    "#         ex_val_acc = len(np.where(ex_predicted_val_output == ex_val_real)[0]) / len(ex_predicted_val_output)\n",
    "#         ex_val_auc_score = roc_auc_score(ex_val_real, ex_val_probability[:, 1])\n",
    "#         ex_val_auc_check = np.append(ex_val_auc_check, ex_val_auc_score)\n",
    "#         ex_val_acc_check = np.append(ex_val_acc_check, ex_val_acc)\n",
    "       \n",
    "    \n",
    "#     if val_auc_check[epoch] == val_auc_check.max():\n",
    "#         print('Validation --------- epoch : {}, validation_loss : {:.6f}'.format(epoch, val_loss_check[epoch]))\n",
    "#         print('Test --------- epoch : {}, test_auc : {:.6f}'.format(epoch, test_auc_check[epoch]))\n",
    "# #         torch.save(my_model.state_dict(), save_root + '/classification_checkpoint.pt')\n",
    "\n",
    "#     if test_acc_check[epoch] >= 0.80:\n",
    "#         conf_matrix = confusion_matrix(test_real, predicted_test_output)\n",
    "#         print(conf_matrix)\n",
    "#         sensitivity = conf_matrix[0, 0] / conf_matrix.sum(axis = 1)[0]\n",
    "#         specificity = conf_matrix[1, 1] / conf_matrix.sum(axis = 1)[1]\n",
    "#         print('sensitivity : ', sensitivity)\n",
    "#         print('specificity : ', specificity)\n",
    "\n",
    "#     if test_auc_check[epoch] == test_auc_check.max():\n",
    "#         print('Test --------- epoch : {}, test_auc : {:.6f}'.format(epoch, test_auc_check[epoch]))\n",
    "\n",
    "#     if val_loss_check[epoch] == val_loss_check.min():\n",
    "#         test_value = test_auc_check[epoch]\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     print('Epoch:[{}]/[{}]\\t'\n",
    "#           'train auc: {:.2f} '\n",
    "#          'acc: {:.2f}\\t '\n",
    "#           'val auc: {:.2f} '\n",
    "#          'acc: {:.2f}\\n'\n",
    "#          'test auc: {:.2f} '\n",
    "#          'acc: {:.2f}\\t'\n",
    "#            'ex_test auc: {:.2f} '\n",
    "#          'acc: {:.2f}\\t'\n",
    "#           'ex_val_auc : {:.2f} '\n",
    "#           'acc : {:.2f}'\n",
    "#           .format(epoch, num_epoch, train_auc_score, train_acc, \n",
    "#                                       val_auc_score, val_acc, test_auc_score, test_acc, \n",
    "#                   ex_test_auc_score, ex_test_acc, ex_val_auc_score, ex_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-macintosh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-rebel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-mapping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-suspension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-inspector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-companion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-tunisia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
